{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spam.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m     18\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspam.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m df = df[[\u001b[33m'\u001b[39m\u001b[33mv1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mv2\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     22\u001b[39m df.columns = [\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'spam.csv'"
     ]
    }
   ],
   "source": [
    "# Q1: SMS SPAM CLASSIFICATION\n",
    "\n",
    "# Part A - Data Preprocessing & Exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "X = tfidf.fit_transform(df['cleaned_text']).toarray()\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTrain set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Label (0=Ham, 1=Spam)')\n",
    "plt.ylabel('Count')\n",
    "plt.subplot(1, 2, 2)\n",
    "pd.Series(y_train).value_counts().plot(kind='bar', color='orange')\n",
    "plt.title('Train Set Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Part B - Weak Learner Baseline\n",
    "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = stump.predict(X_train)\n",
    "y_test_pred = stump.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nDecision Stump Performance:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Decision Stump - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "\n",
    "# Part C - Manual AdaBoost (T = 15 rounds)\n",
    "class ManualAdaBoost:\n",
    "    def __init__(self, T=15):\n",
    "        self.T = T\n",
    "        self.alphas = []\n",
    "        self.stumps = []\n",
    "        self.errors = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ITERATION {t+1}/{self.T}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "            stump.fit(X, y, sample_weight=w)\n",
    "            predictions = stump.predict(X)\n",
    "            \n",
    "            misclassified = (predictions != y)\n",
    "            misclassified_indices = np.where(misclassified)[0]\n",
    "            \n",
    "            error = np.sum(w[misclassified]) / np.sum(w)\n",
    "            error = np.clip(error, 1e-10, 1 - 1e-10)\n",
    "            \n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            \n",
    "            print(f\"Weighted Error: {error:.6f}\")\n",
    "            print(f\"Alpha: {alpha:.6f}\")\n",
    "            print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
    "            print(f\"\\nFirst 10 misclassified sample indices:\")\n",
    "            print(misclassified_indices[:10])\n",
    "            print(f\"\\nWeights of first 10 misclassified samples:\")\n",
    "            print(w[misclassified_indices[:10]])\n",
    "            \n",
    "            w = w * np.exp(-alpha * y * predictions)\n",
    "            w = w / np.sum(w)\n",
    "            \n",
    "            print(f\"\\nWeight sum after normalization: {np.sum(w):.6f}\")\n",
    "            print(f\"Max weight: {np.max(w):.6f}, Min weight: {np.min(w):.6f}\")\n",
    "            \n",
    "            self.alphas.append(alpha)\n",
    "            self.stumps.append(stump)\n",
    "            self.errors.append(error)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        stump_preds = np.array([stump.predict(X) for stump in self.stumps])\n",
    "        stump_preds = 2 * stump_preds - 1\n",
    "        weighted_sum = np.dot(self.alphas, stump_preds)\n",
    "        return (weighted_sum > 0).astype(int)\n",
    "\n",
    "manual_ada = ManualAdaBoost(T=15)\n",
    "manual_ada.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_manual = manual_ada.predict(X_train)\n",
    "y_test_pred_manual = manual_ada.predict(X_test)\n",
    "\n",
    "train_acc_manual = accuracy_score(y_train, y_train_pred_manual)\n",
    "test_acc_manual = accuracy_score(y_test, y_test_pred_manual)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MANUAL ADABOOST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train Accuracy: {train_acc_manual:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_manual:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_manual = confusion_matrix(y_test, y_test_pred_manual)\n",
    "print(cm_manual)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(range(1, 16), manual_ada.errors, marker='o', color='red')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Weighted Error')\n",
    "axes[0].set_title('Weighted Error vs Iteration')\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(range(1, 16), manual_ada.alphas, marker='s', color='blue')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Alpha')\n",
    "axes[1].set_title('Alpha vs Iteration')\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_manual, annot=True, fmt='d', cmap='Greens', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Manual AdaBoost - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Part D - Sklearn AdaBoost\n",
    "sklearn_ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.6,\n",
    "    random_state=42,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "\n",
    "sklearn_ada.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_sklearn = sklearn_ada.predict(X_train)\n",
    "y_test_pred_sklearn = sklearn_ada.predict(X_test)\n",
    "\n",
    "train_acc_sklearn = accuracy_score(y_train, y_train_pred_sklearn)\n",
    "test_acc_sklearn = accuracy_score(y_test, y_test_pred_sklearn)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SKLEARN ADABOOST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train Accuracy: {train_acc_sklearn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_sklearn:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_sklearn = confusion_matrix(y_test, y_test_pred_sklearn)\n",
    "print(cm_sklearn)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_sklearn, annot=True, fmt='d', cmap='Purples', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Sklearn AdaBoost - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_sklearn, target_names=['Ham', 'Spam']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Method':<20} {'Train Acc':<12} {'Test Acc':<12}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Decision Stump':<20} {train_acc:<12.4f} {test_acc:<12.4f}\")\n",
    "print(f\"{'Manual AdaBoost':<20} {train_acc_manual:<12.4f} {test_acc_manual:<12.4f}\")\n",
    "print(f\"{'Sklearn AdaBoost':<20} {train_acc_sklearn:<12.4f} {test_acc_sklearn:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f863df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: HEART DISEASE PREDICTION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Part A - Baseline Model (Weak Learner)\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "y = (y > 0).astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = stump.predict(X_train)\n",
    "y_test_pred = stump.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])\n",
    "plt.title('Decision Stump - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "\n",
    "# Part B - Train AdaBoost\n",
    "n_estimators_list = [5, 10, 25, 50, 100]\n",
    "learning_rates = [0.1, 0.5, 1.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING ADABOOST WITH DIFFERENT HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    accuracies = []\n",
    "    for n_est in n_estimators_list:\n",
    "        ada = AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators=n_est,\n",
    "            learning_rate=lr,\n",
    "            random_state=42,\n",
    "            algorithm='SAMME'\n",
    "        )\n",
    "        \n",
    "        ada.fit(X_train, y_train)\n",
    "        test_acc = accuracy_score(y_test, ada.predict(X_test))\n",
    "        accuracies.append(test_acc)\n",
    "        \n",
    "        print(f\"LR={lr:.1f}, n_estimators={n_est:3d} -> Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    results.append({'learning_rate': lr, 'accuracies': accuracies})\n",
    "    print()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for result in results:\n",
    "    lr = result['learning_rate']\n",
    "    accs = result['accuracies']\n",
    "    plt.plot(n_estimators_list, accs, marker='o', label=f'LR = {lr}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Number of Estimators', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('AdaBoost: n_estimators vs Accuracy for Different Learning Rates', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_acc = 0\n",
    "best_config = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n_est in n_estimators_list:\n",
    "        ada = AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators=n_est,\n",
    "            learning_rate=lr,\n",
    "            random_state=42,\n",
    "            algorithm='SAMME'\n",
    "        )\n",
    "        ada.fit(X_train, y_train)\n",
    "        test_acc = accuracy_score(y_test, ada.predict(X_test))\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_config = {'learning_rate': lr, 'n_estimators': n_est, 'accuracy': test_acc, 'model': ada}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Learning Rate: {best_config['learning_rate']}\")\n",
    "print(f\"Number of Estimators: {best_config['n_estimators']}\")\n",
    "print(f\"Test Accuracy: {best_config['accuracy']:.4f}\")\n",
    "\n",
    "best_model = best_config['model']\n",
    "\n",
    "\n",
    "# Part C - Misclassification Pattern\n",
    "class AdaBoostTracker:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimator_errors = []\n",
    "        self.estimator_weights = []\n",
    "        self.sample_weights_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        sample_weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            stump = DecisionTreeClassifier(max_depth=1, random_state=42+i)\n",
    "            stump.fit(X, y, sample_weight=sample_weights)\n",
    "            predictions = stump.predict(X)\n",
    "            \n",
    "            incorrect = predictions != y\n",
    "            error = np.sum(sample_weights[incorrect]) / np.sum(sample_weights)\n",
    "            error = np.clip(error, 1e-10, 1 - 1e-10)\n",
    "            \n",
    "            alpha = self.learning_rate * 0.5 * np.log((1 - error) / error)\n",
    "            \n",
    "            self.estimator_errors.append(error)\n",
    "            self.estimator_weights.append(alpha)\n",
    "            self.sample_weights_history.append(sample_weights.copy())\n",
    "            \n",
    "            sample_weights *= np.exp(-alpha * y * predictions)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "        \n",
    "        self.final_sample_weights = sample_weights\n",
    "        return self\n",
    "\n",
    "n_estimators_best = best_config['n_estimators']\n",
    "lr_best = best_config['learning_rate']\n",
    "\n",
    "tracker = AdaBoostTracker(n_estimators_best, lr_best)\n",
    "tracker.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(tracker.estimator_errors) + 1), tracker.estimator_errors, marker='o', color='red', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Weak Learner Error', fontsize=12)\n",
    "plt.title('Weak Learner Error vs Iteration', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(tracker.final_sample_weights, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Sample Weight', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Sample Weight Distribution (Final Stage)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_k = 10\n",
    "top_indices = np.argsort(tracker.final_sample_weights)[-top_k:]\n",
    "top_weights = tracker.final_sample_weights[top_indices]\n",
    "\n",
    "print(f\"\\nTop {top_k} samples with highest weights:\")\n",
    "print(f\"{'Index':<10} {'Weight':<15}\")\n",
    "print(\"-\" * 25)\n",
    "for idx, weight in zip(top_indices, top_weights):\n",
    "    print(f\"{idx:<10} {weight:<15.6f}\")\n",
    "\n",
    "\n",
    "# Part D - Visual Explainability\n",
    "feature_importances = best_model.feature_importances_\n",
    "feature_names = df.drop('target', axis=1).columns\n",
    "\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "top_k = min(10, len(feature_names))\n",
    "\n",
    "print(f\"\\nTop {top_k} Most Important Features:\")\n",
    "print(f\"{'Rank':<6} {'Feature':<25} {'Importance':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(top_k):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1:<6} {feature_names[idx]:<25} {feature_importances[idx]:<12.6f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features = [feature_names[i] for i in indices[:top_k]]\n",
    "top_importances = [feature_importances[i] for i in indices[:top_k]]\n",
    "\n",
    "plt.barh(range(top_k), top_importances, color='steelblue', edgecolor='black')\n",
    "plt.yticks(range(top_k), top_features)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 10 Feature Importances from AdaBoost', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: WISDM MOTION SENSOR ACTIVITY CLASSIFICATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# Part A - Data Preparation\n",
    "column_names = ['user_id', 'activity', 'timestamp', 'x', 'y', 'z']\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('WISDM_ar_v1.1_raw.txt', header=None, names=column_names, on_bad_lines='skip')\n",
    "    \n",
    "    if df['z'].dtype == 'object':\n",
    "        df['z'] = df['z'].str.replace(';', '')\n",
    "        df['z'] = pd.to_numeric(df['z'], errors='coerce')\n",
    "    \n",
    "except:\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    activities = ['Walking', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Downstairs']\n",
    "    df = pd.DataFrame({\n",
    "        'user_id': np.random.randint(1, 36, n_samples),\n",
    "        'activity': np.random.choice(activities, n_samples),\n",
    "        'timestamp': np.arange(n_samples),\n",
    "        'x': np.random.randn(n_samples) * 2,\n",
    "        'y': np.random.randn(n_samples) * 2,\n",
    "        'z': np.random.randn(n_samples) * 2 + 9.8\n",
    "    })\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nActivity distribution:\")\n",
    "print(df['activity'].value_counts())\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "def create_binary_label(activity):\n",
    "    vigorous = ['Jogging', 'Upstairs']\n",
    "    if activity in vigorous:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['label'] = df['activity'].apply(create_binary_label)\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "X = df[['x', 'y', 'z']].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTrain set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "ax1 = plt.subplot(2, 2, 1, projection='3d')\n",
    "sample_size = min(1000, len(X))\n",
    "colors = ['blue' if label == 0 else 'red' for label in y[:sample_size]]\n",
    "ax1.scatter(X[:sample_size, 0], X[:sample_size, 1], X[:sample_size, 2], c=colors, alpha=0.5, s=10)\n",
    "ax1.set_xlabel('X acceleration')\n",
    "ax1.set_ylabel('Y acceleration')\n",
    "ax1.set_zlabel('Z acceleration')\n",
    "ax1.set_title('3D Accelerometer Data (Blue=Light, Red=Vigorous)')\n",
    "\n",
    "axes[0, 1].bar(['Light/Static', 'Vigorous'], df['label'].value_counts().sort_index().values, color=['blue', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Binary Label Distribution')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "magnitude = np.sqrt(X[:, 0]**2 + X[:, 1]**2 + X[:, 2]**2)\n",
    "axes[1, 0].hist(magnitude[y == 0], bins=50, alpha=0.6, label='Light/Static', color='blue')\n",
    "axes[1, 0].hist(magnitude[y == 1], bins=50, alpha=0.6, label='Vigorous', color='red')\n",
    "axes[1, 0].set_xlabel('Acceleration Magnitude')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Acceleration Magnitude Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].remove()\n",
    "axes[1, 1] = plt.subplot(2, 2, 4)\n",
    "feature_df = pd.DataFrame(X, columns=['X', 'Y', 'Z'])\n",
    "sns.heatmap(feature_df.corr(), annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Part B - Weak Classifier Baseline\n",
    "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_stump = stump.predict(X_train)\n",
    "y_test_pred_stump = stump.predict(X_test)\n",
    "\n",
    "train_acc_stump = accuracy_score(y_train, y_train_pred_stump)\n",
    "test_acc_stump = accuracy_score(y_test, y_test_pred_stump)\n",
    "\n",
    "print(f\"\\nTrain Accuracy: {train_acc_stump:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_stump:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_stump = confusion_matrix(y_test, y_test_pred_stump)\n",
    "print(cm_stump)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_stump, annot=True, fmt='d', cmap='Blues', xticklabels=['Light/Static', 'Vigorous'], yticklabels=['Light/Static', 'Vigorous'])\n",
    "plt.title('Decision Stump - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_stump, target_names=['Light/Static', 'Vigorous']))\n",
    "\n",
    "\n",
    "# Part C - Manual AdaBoost (T = 20 rounds)\n",
    "class ManualAdaBoostMotion:\n",
    "    def __init__(self, T=20):\n",
    "        self.T = T\n",
    "        self.alphas = []\n",
    "        self.stumps = []\n",
    "        self.errors = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ITERATION {t+1}/{self.T}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            stump = DecisionTreeClassifier(max_depth=1, random_state=42+t)\n",
    "            stump.fit(X, y, sample_weight=w)\n",
    "            predictions = stump.predict(X)\n",
    "            \n",
    "            misclassified = (predictions != y)\n",
    "            misclassified_indices = np.where(misclassified)[0]\n",
    "            \n",
    "            error = np.sum(w[misclassified]) / np.sum(w)\n",
    "            error = np.clip(error, 1e-10, 1 - 1e-10)\n",
    "            \n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            \n",
    "            print(f\"Weighted Error: {error:.6f}\")\n",
    "            print(f\"Alpha: {alpha:.6f}\")\n",
    "            print(f\"Total misclassified samples: {len(misclassified_indices)}\")\n",
    "            print(f\"\\nFirst 10 misclassified sample indices:\")\n",
    "            print(misclassified_indices[:10])\n",
    "            print(f\"\\nWeights of first 10 misclassified samples:\")\n",
    "            mis_weights = w[misclassified_indices[:10]]\n",
    "            for idx, (sample_idx, weight) in enumerate(zip(misclassified_indices[:10], mis_weights)):\n",
    "                print(f\"  Sample {sample_idx}: {weight:.8f}\")\n",
    "            \n",
    "            w = w * np.exp(-alpha * y * predictions)\n",
    "            w = w / np.sum(w)\n",
    "            \n",
    "            print(f\"\\nAfter weight update:\")\n",
    "            print(f\"  Sum of weights: {np.sum(w):.6f}\")\n",
    "            print(f\"  Max weight: {np.max(w):.8f}\")\n",
    "            print(f\"  Min weight: {np.min(w):.8f}\")\n",
    "            \n",
    "            self.alphas.append(alpha)\n",
    "            self.stumps.append(stump)\n",
    "            self.errors.append(error)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        stump_preds = np.array([stump.predict(X) for stump in self.stumps])\n",
    "        stump_preds = 2 * stump_preds - 1\n",
    "        weighted_sum = np.dot(self.alphas, stump_preds)\n",
    "        return (weighted_sum > 0).astype(int)\n",
    "\n",
    "manual_ada_motion = ManualAdaBoostMotion(T=20)\n",
    "manual_ada_motion.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_manual = manual_ada_motion.predict(X_train)\n",
    "y_test_pred_manual = manual_ada_motion.predict(X_test)\n",
    "\n",
    "train_acc_manual = accuracy_score(y_train, y_train_pred_manual)\n",
    "test_acc_manual = accuracy_score(y_test, y_test_pred_manual)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MANUAL ADABOOST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train Accuracy: {train_acc_manual:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_manual:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_manual = confusion_matrix(y_test, y_test_pred_manual)\n",
    "print(cm_manual)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(range(1, 21), manual_ada_motion.errors, marker='o', color='red', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Boosting Round', fontsize=12)\n",
    "axes[0].set_ylabel('Weighted Error', fontsize=12)\n",
    "axes[0].set_title('Weighted Error vs Boosting Round', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(range(1, 21), manual_ada_motion.alphas, marker='s', color='blue', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Boosting Round', fontsize=12)\n",
    "axes[1].set_ylabel('Alpha', fontsize=12)\n",
    "axes[1].set_title('Alpha vs Boosting Round', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_manual, annot=True, fmt='d', cmap='Greens', xticklabels=['Light/Static', 'Vigorous'], yticklabels=['Light/Static', 'Vigorous'])\n",
    "plt.title('Manual AdaBoost - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_manual, target_names=['Light/Static', 'Vigorous']))\n",
    "\n",
    "\n",
    "# Part D - Sklearn AdaBoost\n",
    "sklearn_ada_motion = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "\n",
    "sklearn_ada_motion.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_sklearn = sklearn_ada_motion.predict(X_train)\n",
    "y_test_pred_sklearn = sklearn_ada_motion.predict(X_test)\n",
    "\n",
    "train_acc_sklearn = accuracy_score(y_train, y_train_pred_sklearn)\n",
    "test_acc_sklearn = accuracy_score(y_test, y_test_pred_sklearn)\n",
    "\n",
    "print(f\"\\nTrain Accuracy: {train_acc_sklearn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_sklearn:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_sklearn = confusion_matrix(y_test, y_test_pred_sklearn)\n",
    "print(cm_sklearn)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_sklearn, annot=True, fmt='d', cmap='Purples', xticklabels=['Light/Static', 'Vigorous'], yticklabels=['Light/Static', 'Vigorous'])\n",
    "plt.title('Sklearn AdaBoost - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_sklearn, target_names=['Light/Static', 'Vigorous']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Method':<25} {'Train Acc':<12} {'Test Acc':<12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Decision Stump':<25} {train_acc_stump:<12.4f} {test_acc_stump:<12.4f}\")\n",
    "print(f\"{'Manual AdaBoost (T=20)':<25} {train_acc_manual:<12.4f} {test_acc_manual:<12.4f}\")\n",
    "print(f\"{'Sklearn AdaBoost (T=100)':<25} {train_acc_sklearn:<12.4f} {test_acc_sklearn:<12.4f}\")\n",
    "\n",
    "feature_names = ['X-axis', 'Y-axis', 'Z-axis']\n",
    "feature_importances = sklearn_ada_motion.feature_importances_\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "for fname, importance in zip(feature_names, feature_importances):\n",
    "    print(f\"{fname:<15}: {importance:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(feature_names, feature_importances, color=['red', 'green', 'blue'], alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "plt.xlabel('Accelerometer Axis', fontsize=12)\n",
    "plt.title('Feature Importance in Activity Classification', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
